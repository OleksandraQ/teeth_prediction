{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Hardness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch package version 1.0.1 was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "import shap # https://github.com/slundberg/shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred, mean_, var_): \n",
    "    y_true, y_pred = np.array(y_true)* var_ +mean_, np.array(y_pred)* var_ +mean_\n",
    "    res = []\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] != 0.:\n",
    "            res.append(np.abs((y_true[i] - y_pred[i]) / y_true[i]))\n",
    "    return np.mean(res) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_errors(test_data, predicted_data, mean_, var_):\n",
    "    \"\"\" Different error estimates.\n",
    "    \"\"\"\n",
    "    # Total variance\n",
    "    SSt_DNN = np.sum(np.square(test_data-np.mean(test_data)))\n",
    "    # Residual sum of squares\n",
    "    SSr_DNN = np.sum(np.square(predicted_data-test_data))\n",
    "    # Root-mean-square error\n",
    "    RMSE_DNN = np.sqrt(np.sum(np.square(predicted_data-test_data)))\n",
    "    # R^2 coefficient\n",
    "    r2_DNN = 1-(SSr_DNN/SSt_DNN)\n",
    "    MAPE = mean_absolute_percentage_error(test_data, predicted_data, mean_, var_)\n",
    "    return RMSE_DNN, r2_DNN, MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_X_perm(X_orig, data_to_select, feature_ind):\n",
    "    \"\"\" Helper function to generate random data for importance check\n",
    "    \"\"\"\n",
    "    new_X = np.copy(X_orig)\n",
    "    mu, sigma = np.mean(X_orig[:,feature_ind]), np.std(X_orig[:,feature_ind])\n",
    "    new_X[:,feature_ind] = np.random.normal(mu, sigma, X_orig.shape[0])\n",
    "    return new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\n",
    "    Based on https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, patience=7, verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int, default = 7): \n",
    "                How long to wait after last time validation loss improved.\n",
    "            verbose (bool, default = False): \n",
    "                If True, prints a message for each validation loss improvement. \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print('Validation loss decreased ('+str(self.val_loss_min)+' --> '+str(val_loss)+'.  Saving model ...')\n",
    "        torch.save(model.state_dict(), 'checkpoint_.pt')\n",
    "        self.val_loss_min = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    \"\"\" Neural network architecture used for prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.hidden(x))      # activation function for hidden layer\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "row"
   },
   "outputs": [],
   "source": [
    "def train_and_val(X, Y, n_neurons, mean_ = 0., var_ = 1., test_size =0.2, silent = False):\n",
    "    \"\"\" Function to train and validate the neural network \n",
    "        of a class Net (linear input and output activation functions,\n",
    "        one hidden layer with tanh).\n",
    "    \n",
    "    Args:\n",
    "        X (numpy array): \n",
    "            normalized input for training and validation;\n",
    "        Y (numpy array):\n",
    "            corresponding output for training and validation;\n",
    "        n_neurons (int):\n",
    "            the size of a hidden layer;\n",
    "        mean_ (float, default = 0.):\n",
    "            mean value of the training (output) data if it was normalized at the beginning; \n",
    "        var_ (float, default = 1.):\n",
    "            standard deviation of the training (output) data if it was normalized at the beginning;             \n",
    "        test_size (float, default = 0.2): \n",
    "            which percent of the data to use for validation (values between 0.0 and 1.0);\n",
    "        silent (bool, default = False): \n",
    "            If True, prints messages.\n",
    "    Returns:\n",
    "        Net: \n",
    "            trained neural network;\n",
    "        float:\n",
    "            R^2 coefficient for validation data;\n",
    "        float:\n",
    "            value of loss function for the validation data;\n",
    "        float:\n",
    "            value of loss function for the training data;\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size)\n",
    "\n",
    "    y_train=y_train.reshape(y_train.size,1)\n",
    "\n",
    "    y_test=y_test.reshape(y_test.size,1)\n",
    "    \n",
    "    x = torch.from_numpy(X_train).float()\n",
    "    y = torch.from_numpy(y_train).float()\n",
    "\n",
    "    x_val = torch.from_numpy(X_test).float()\n",
    "    y_val = torch.from_numpy(y_test).float()\n",
    "\n",
    "    net = Net(n_feature=X_train.shape[1], n_hidden=n_neurons, n_output=y_train.shape[1]) # define the network\n",
    "    if not silent: print(net)  # net architecture\n",
    "\n",
    "    loss_func = torch.nn.SmoothL1Loss()  # loss function\n",
    "    optimizer = optim.LBFGS(net.parameters(), lr=0.8) # optimization algorithm\n",
    "\n",
    "    plt.ion()   # something about plotting\n",
    "\n",
    "    loss_train = []\n",
    "    loss_test = []\n",
    "\n",
    "    avg_train_losses = []\n",
    "    # to track the average validation loss per epoch as the model trains\n",
    "    avg_valid_losses = [] \n",
    "    patience = 5\n",
    "    n_epochs = 20\n",
    "    early_stopping = EarlyStopping(patience=patience)\n",
    "    for t in range(n_epochs):\n",
    "        if not silent: print('STEP: ', t)\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            out = net(x)\n",
    "            loss = loss_func(out, y)\n",
    "            #print('loss:', loss.item())\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        optimizer.step(closure)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction = net(x_val)\n",
    "            loss = loss_func(prediction, y_val)\n",
    "            pr = net(x)\n",
    "            loss_tr = loss_func(pr, y)\n",
    "            loss_train.append(loss_tr.item())\n",
    "            if not silent: print('train loss:', loss_tr.item())\n",
    "            loss_test.append(loss.item())\n",
    "            if not silent: print('test loss:', loss.item())\n",
    "\n",
    "        if not silent: print(\"[ \"+str(t)+\"/\"+str(n_epochs)+\"] train_loss: \"+str(loss_train[t])+ \" valid_loss: \"+str(loss_test[t]))\n",
    "\n",
    "        # early_stopping needs the validation loss to check if it has decreased, \n",
    "        # and if it has, it will make a checkpoint of the current model\n",
    "        early_stopping(loss_test[t], net)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "                if not silent: print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "        \n",
    "\n",
    "        if t % 1 == 0:\n",
    "            # plot and show learning process at each step (replace 1 with other number of steps)\n",
    "            if not silent:\n",
    "                plt.cla()\n",
    "                plt.plot(y_val.data.numpy()[:100])\n",
    "                plt.plot(prediction.data.numpy()[:100])\n",
    "                plt.show()\n",
    "                plt.plot(y.data.numpy()[:200])\n",
    "                plt.plot(pr.data.numpy()[:200])\n",
    "                plt.show()\n",
    "                plt.scatter(y_val.data.numpy(), prediction.data.numpy())\n",
    "                plt.text(0.5, 0, 'Loss=%.4f' % loss.data.numpy(), fontdict={'size': 20, 'color':  'red'})\n",
    "                plt.pause(0.1)\n",
    "    if not silent:\n",
    "        plt.ioff()\n",
    "        plt.show()\n",
    "    \n",
    "    _, r2, _ = get_errors(y_val.data.numpy(), prediction.data.numpy(), mean_, var_)\n",
    "    \n",
    "    # load the last checkpoint with the best model\n",
    "    net.load_state_dict(torch.load('checkpoint_.pt'))\n",
    "\n",
    "    l_v = loss_func(net(x_val), y_val).item()\n",
    "    l_tr = loss_func(net(x), y).item()\n",
    "\n",
    "    \n",
    "    return net, r2, l_v, l_tr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_all_tooth(data_path, elements_names, model):\n",
    "    \"\"\" Predict the whole tooth surface using trained network.\n",
    "    \n",
    "    Args:\n",
    "            data_path (str): \n",
    "                path to the measurements of the elements content. \n",
    "                Files must be named element_name.txt (e.g. Ca.txt)\n",
    "            elements_names (list): \n",
    "                the list of strings with names of elements\n",
    "            model (Net): \n",
    "                trained neural network\n",
    "            \n",
    "    Returns:\n",
    "            numpy array: \n",
    "                prediction reshaped to the tooth shape\n",
    "            numpy array: \n",
    "                not reshaped predictions           \n",
    "    \n",
    "    \"\"\"\n",
    "    #read all data\n",
    "    read_data = []\n",
    "    for c in elements_names:\n",
    "        data = np.genfromtxt(data_path+str(c)+\".txt\" )\n",
    "        size = data.shape\n",
    "        read_data.append(data.ravel())\n",
    "    \n",
    "    read_data = np.array(read_data).T\n",
    "    \n",
    "    data_mask = np.zeros(read_data.shape[0])\n",
    "    read_data_new = []\n",
    "    for i in range(len(read_data)):\n",
    "        if len(np.where(read_data[i] > 0.)[0]) > 3:\n",
    "            read_data_new.append(read_data[i])\n",
    "            data_mask[i] = 1\n",
    "            \n",
    "    read_data_new = (np.array(read_data_new) - mean_X)/var_X\n",
    "    \n",
    "    y_p = model(torch.from_numpy(read_data_new).float())\n",
    "    \n",
    "    y_pr = y_p.data.numpy()\n",
    "        \n",
    "    rebuilded = np.zeros(read_data.shape[0])\n",
    "    count = 0\n",
    "    for i in range(len(data_mask)):\n",
    "        if data_mask[i] != 0: \n",
    "            rebuilded[i] = y_pr[count]\n",
    "            count += 1  \n",
    "            \n",
    "    rebuilded_new = rebuilded.reshape(size)\n",
    "    \n",
    "    return rebuilded_new, y_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pred_results(save_folder, array_pred, N, species_name):\n",
    "    \"\"\" Helper function to save the results of predictions. \"\"\"\n",
    "    \n",
    "    for i in range(len(array_pred)):\n",
    "        try:\n",
    "            os.mkdir(save_folder+\"pred_\"+species_name) # create folder for saving the predictions\n",
    "        except: \n",
    "            pass\n",
    "        np.savetxt(save_folder+\"pred_\"+species_name+\"/pred_\"+str(i)+\"_N_\"+str(N)+\".txt\", array_pred[i], fmt='%.7f', delimiter=',')\n",
    "    \n",
    "    np.savetxt(save_folder+\"mean_pred_\"+species_name+\"_N_\"+str(N)+\".txt\", np.mean(array_pred[np.where(R2_array > 0.7)[0]],axis=0), fmt='%.7f', delimiter=',')\n",
    "    np.savetxt(save_folder+\"std_pred_\"+species_name+\"_N_\"+str(N)+\".txt\", np.std(array_pred[np.where(R2_array > 0.7)[0]],axis=0), fmt='%.7f', delimiter=',')\n",
    "    \n",
    "    plt.figure(figsize = (10,10))\n",
    "    plt.imshow(np.mean(array_pred[np.where(R2_array > 0.7)[0]],axis=0), cmap=plt.cm.get_cmap(\"jet\", 20))\n",
    "    plt.clim(0, max(Y_H_))\n",
    "    plt.colorbar()\n",
    "    plt.savefig(save_folder+'mean_pred_'+species_name+'_N_'+str(N)+'_20colors.png', dpi=300)\n",
    "    \n",
    "    plt.figure(figsize = (10,10))\n",
    "    plt.imshow(np.mean(array_pred[np.where(R2_array > 0.7)[0]],axis=0), cmap=plt.cm.get_cmap(\"jet\", 20))\n",
    "    plt.colorbar()\n",
    "    plt.savefig(save_folder+'mean_pred_'+species_name+'_N_'+str(N)+'_20colors_nomax.png', dpi=300)\n",
    "    \n",
    "    plt.figure(figsize = (10,10))\n",
    "    plt.imshow(np.mean(array_pred[np.where(R2_array > 0.7)[0]],axis=0), cmap=plt.cm.get_cmap(\"jet\", 6))\n",
    "    plt.clim(0, max(Y_H_))\n",
    "    plt.colorbar()\n",
    "    plt.savefig(save_folder+'mean_pred_'+species_name+'_N_'+str(N)+'_6colors.png', dpi=300)\n",
    "    \n",
    "    plt.figure(figsize = (10,10))\n",
    "    plt.imshow(np.mean(array_pred[np.where(R2_array > 0.7)[0]],axis=0), cmap=plt.cm.get_cmap(\"jet\", 6))\n",
    "    plt.colorbar()\n",
    "    plt.savefig(save_folder+'mean_pred_'+species_name+'_N_'+str(N)+'_6colors_nomax.png', dpi=300)\n",
    "    \n",
    "    plt.figure(figsize = (10,10))\n",
    "    plt.imshow(np.std(array_pred[np.where(R2_array > 0.7)[0]],axis=0), cmap=plt.cm.get_cmap(\"jet\", 20))\n",
    "    plt.colorbar()\n",
    "    plt.savefig(save_folder+'std_pred_'+species_name+'_N_'+str(N)+'_20colors.png', dpi=300)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Using ensembles of neural networks (NN) to predict Hardness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data for training and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "row"
   },
   "outputs": [],
   "source": [
    "elements_names = ['C', 'Ca', 'F', 'Fe', 'Mg', 'Na', 'O', 'P']\n",
    "species_list = [\"AAl\", \"HCh\", \"HIs\", \"HNy\", \"HSa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the training data\n",
    "source = path+'/Data/Hardness_Elasticity_EDX_combo_new.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the elements measurement used for prediction\n",
    "data_folder_all = path+'/Data/ralf_smooth/'\n",
    "AAL_data = data_folder_all+'AAl/'\n",
    "HCh_data = data_folder_all+'HCh/'\n",
    "HIs_data = data_folder_all+'HIs/'\n",
    "HNy_data = data_folder_all+'HNy/'\n",
    "HSa_data = data_folder_all+'HSa/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = np.genfromtxt(source, delimiter=',', skip_header=1, dtype=float, usecols=(range(5,15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_H_ = data_all[:,0] # hardness\n",
    "#Y_E_ = data_all[:,1] # elastic modulus\n",
    "X_ = data_all[:,2:]\n",
    "mean_H = np.mean(Y_H_)\n",
    "var_H = np.std(Y_H_)\n",
    "# mean_E = np.mean(Y_E_)\n",
    "# var_E = np.std(Y_E_)\n",
    "Y_H = (Y_H_- mean_H)/var_H\n",
    "# Y_E = (Y_E_- mean_E)/var_E\n",
    "mean_X = np.mean(X_,axis=0)\n",
    "var_X = np.std(X_,axis=0)\n",
    "X = (X_- mean_X)/var_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder, where to save the results\n",
    "save_folder = path+\"/result_H_/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(save_folder)\n",
    "except OSError:\n",
    "    print (\"Creation of the directory %s failed\" % save_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and saving NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train 500 independent NNs, save them. Each individual predictions are also saved. Only the networks with $R^2 > 0.7$ will be used for final ensemble prediction. The analysis was repeated for different number of neurons (5, 8, 10, 12, 15, 18, 20) and then the performance was compared. The final results are obtained with the 20 neurons in a hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns = [5, 8, 10, 12, 15, 18, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_neurons in Ns:\n",
    "    R_2 = []\n",
    "    nets = []\n",
    "    Loss_v = []\n",
    "    pred = []\n",
    "    for i in range(500):\n",
    "        # train 500 networks\n",
    "        net1, R2, loss_v, loss_tr = train_and_val(X, Y_H, n_neurons, mean_H, var_H, test_size =0.2, silent = True)\n",
    "        # add networks, R^2, loss values for validation \n",
    "        nets.append(net1)\n",
    "        R_2.append(R2)\n",
    "        Loss_v.append(loss_v)\n",
    "        try:\n",
    "            os.mkdir(save_folder+\"nets\") # create folder for saving the networks\n",
    "        except: \n",
    "            pass\n",
    "        torch.save(net1.state_dict(), save_folder+\"nets/net_\"+str(i)+\"_N_\"+str(n_neurons))\n",
    "\n",
    "    R2_array = np.array(R_2)\n",
    "    Loss_v_array = np.array(Loss_v)\n",
    "    pr_array = np.array(pred)\n",
    "\n",
    "    np.savetxt(save_folder+\"N_\"+str(n_neurons)+\"_R2_Loss.txt\", np.vstack((R2_array, Loss_v_array)).T, fmt='%.7f', delimiter=',', header=\"R2, Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined R2 and Loss performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2_files = []\n",
    "Loss_files = []\n",
    "for i in Ns:\n",
    "    tmp = np.genfromtxt(save_folder+'N_'+str(i)+'_R2_Loss.txt', skip_header=1, dtype=float, delimiter=',')\n",
    "    R2_files.append(tmp[:,0])\n",
    "    Loss_files.append(tmp[:,1])\n",
    "R2_all = np.array(R2_files)\n",
    "Loss_all = np.array(Loss_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the values for the failed NN for nicer plots\n",
    "R2_all[R2_all == -np.inf] = 0.\n",
    "R2_all[np.isnan(R2_all)]=0.\n",
    "R2_all[np.where(R2_all < 0) ] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the values for the failed NN for nicer plots\n",
    "Loss_all[np.isnan(Loss_all)]=0.3\n",
    "Loss_all[np.where(Loss_all > 1) ] = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "plt.subplot(121)\n",
    "plt.boxplot(R2_all.T)\n",
    "plt.title(r'$R^2$ score for 500 runs', fontsize=20)\n",
    "plt.xlabel(\"Number of neurons in hidden layer\", fontsize=16)\n",
    "plt.ylabel(r'$R^2$', fontsize=16)\n",
    "plt.xticks(np.arange(len(Ns))+1, Ns, fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.boxplot(Loss_all.T)\n",
    "plt.title(r'Smooth $L_1$ loss value for 500 runs', fontsize=20)\n",
    "plt.xlabel(\"Number of neurons in hidden layer\", fontsize=16)\n",
    "plt.ylabel(r'Smooth $L_1$ loss', fontsize=16)\n",
    "plt.xticks(np.arange(len(Ns))+1, Ns, fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "\n",
    "plt.savefig(save_folder+'R2_Loss_H.png', dpi=300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting and saving teeth surfaces for each species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_AAl = []\n",
    "pred_HCh = []\n",
    "pred_HIs = []\n",
    "pred_HNy = []\n",
    "pred_HSa = []\n",
    "\n",
    "for net1 in nets:\n",
    "    \n",
    "    test_rebuilded_new, pr =  predict_all_tooth(AAL_data, elements_names, net1)\n",
    "    inds = np.where(test_rebuilded_new != 0)\n",
    "    test_rebuilded_new[inds] = test_rebuilded_new[inds]*var_H + mean_H\n",
    "    pred_AAl.append(test_rebuilded_new)\n",
    "    \n",
    "    test_rebuilded_new, pr =  predict_all_tooth(HCh_data, elements_names, net1)\n",
    "    inds = np.where(test_rebuilded_new != 0)\n",
    "    test_rebuilded_new[inds] = test_rebuilded_new[inds]*var_H + mean_H\n",
    "    pred_HCh.append(test_rebuilded_new)\n",
    "    \n",
    "    test_rebuilded_new, pr =  predict_all_tooth(HIs_data, elements_names, net1)\n",
    "    inds = np.where(test_rebuilded_new != 0)\n",
    "    test_rebuilded_new[inds] = test_rebuilded_new[inds]*var_H + mean_H\n",
    "    pred_HIs.append(test_rebuilded_new)\n",
    "    \n",
    "    test_rebuilded_new, pr =  predict_all_tooth(HNy_data, elements_names, net1)\n",
    "    inds = np.where(test_rebuilded_new != 0)\n",
    "    test_rebuilded_new[inds] = test_rebuilded_new[inds]*var_H + mean_H\n",
    "    pred_HNy.append(test_rebuilded_new)\n",
    "    \n",
    "    test_rebuilded_new, pr =  predict_all_tooth(HSa_data, elements_names, net1)\n",
    "    inds = np.where(test_rebuilded_new != 0)\n",
    "    test_rebuilded_new[inds] = test_rebuilded_new[inds]*var_H + mean_H\n",
    "    pred_HSa.append(test_rebuilded_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pred_results(save_folder, np.array(pred_AAl), n_neurons, \"AAl\")\n",
    "save_pred_results(save_folder, np.array(pred_HCh), n_neurons, \"HCh\")\n",
    "save_pred_results(save_folder, np.array(pred_HIs), n_neurons, \"HIs\")\n",
    "save_pred_results(save_folder, np.array(pred_HNy), n_neurons, \"HNy\")\n",
    "save_pred_results(save_folder, np.array(pred_HSa), n_neurons, \"HSa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make plots of teeth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read saved data from save_folder\n",
    "mean_arr = []\n",
    "std_arr = []\n",
    "n_neur = 20\n",
    "for sp in species_list:\n",
    "    mean_arr.append(np.genfromtxt(save_folder+\"mean_pred_\"+sp+\"_N_\"+str(n_neur)+\".txt\", delimiter=','))\n",
    "    std_arr.append(np.genfromtxt(save_folder+\"std_pred_\"+sp+\"_N_\"+str(n_neur)+\".txt\", delimiter=','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for colorbar boundaries\n",
    "Max_std = np.max(np.array(std_arr))\n",
    "Max_mean = np.max(np.array(mean_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_sc_pred = \"jet\"\n",
    "color_sc_std = \"afmhot_r\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rc('axes',edgecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fig = plt.figure(figsize = (18,10))\n",
    "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(20, 10))\n",
    "matplotlib.rc('axes',edgecolor='w')\n",
    "\n",
    "# AAl\n",
    "plt.subplot(2,5,1)\n",
    "plt.imshow(mean_arr[0], cmap=plt.cm.get_cmap(color_sc_pred, 20))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.clim(0, 6)\n",
    "plt.xlabel('A. alluaudi', fontsize=20)\n",
    "plt.ylabel('Predicted hardness', fontsize=20)\n",
    "\n",
    "plt.subplot(2,5,6)\n",
    "plt.imshow(std_arr[0], cmap=plt.cm.get_cmap(color_sc_std, 20))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.clim(0, Max_std)\n",
    "plt.ylabel('Standard deviation \\n in predictions', fontsize=20)\n",
    "\n",
    "# HCh\n",
    "plt.subplot(2,5,2)\n",
    "plt.imshow(mean_arr[1], cmap=plt.cm.get_cmap(color_sc_pred, 20))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.clim(0, 6)\n",
    "plt.xlabel('H.chilotos', fontsize=20)\n",
    "\n",
    "plt.subplot(2,5,7)\n",
    "plt.imshow(std_arr[1], cmap=plt.cm.get_cmap(color_sc_std, 20))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.clim(0, Max_std)\n",
    "\n",
    "# HIs\n",
    "plt.subplot(2,5,3)\n",
    "plt.imshow(mean_arr[2], cmap=plt.cm.get_cmap(color_sc_pred, 20))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.clim(0, 6)\n",
    "plt.xlabel('H.ishmaeli', fontsize=20)\n",
    "\n",
    "plt.subplot(2,5,8)\n",
    "plt.imshow(std_arr[2], cmap=plt.cm.get_cmap(color_sc_std, 20))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.clim(0, Max_std)\n",
    "\n",
    "# HNy\n",
    "plt.subplot(2,5,4)\n",
    "plt.imshow(mean_arr[3], cmap=plt.cm.get_cmap(color_sc_pred, 20))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.clim(0, 6)\n",
    "plt.xlabel('P. nyererei', fontsize=20)\n",
    "\n",
    "plt.subplot(2,5,9)\n",
    "plt.imshow(std_arr[3], cmap=plt.cm.get_cmap(color_sc_std, 20))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.clim(0, Max_std)\n",
    "\n",
    "# HSa\n",
    "plt.subplot(2,5,5)\n",
    "im1 = plt.imshow(mean_arr[4], cmap=plt.cm.get_cmap(color_sc_pred, 20))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.clim(0, 6)\n",
    "#plt.colorbar()\n",
    "plt.xlabel('H. sauvagei', fontsize=20)\n",
    "\n",
    "plt.subplot(2,5,10)\n",
    "im2 = plt.imshow(std_arr[4], cmap=plt.cm.get_cmap(color_sc_std, 20))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.clim(0, Max_std)\n",
    "#plt.colorbar()\n",
    "\n",
    "cax = plt.axes([0.91, 0.537, 0.02, 0.343])\n",
    "cbar = plt.colorbar(im1, cax=cax)\n",
    "cbar.ax.tick_params(labelsize=16) \n",
    "cbar.set_label(r\"[GPa]\", size=16)\n",
    "\n",
    "cax = plt.axes([0.91, 0.125, 0.02, 0.343])\n",
    "cbar = plt.colorbar(im2, cax=cax)\n",
    "cbar.ax.tick_params(labelsize=16) \n",
    "cbar.set_label(r\"[GPa]\", size=16)\n",
    "\n",
    "\n",
    "plt.savefig(save_folder+'all_H.png', dpi=300, bbox_inches = 'tight')\n",
    "plt.show()\n",
    "matplotlib.rc('axes',edgecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Define the influence of elements on the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved R^2 values to define the network with maximum R^2\n",
    "n_neur = 20\n",
    "R2_orig = np.genfromtxt(save_folder+'N_'+str(n_neur)+'_R2_Loss.txt', skip_header=1, dtype=float, delimiter=',', usecols=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the index of the network with maximum R^2\n",
    "good_NN_inds = np.where(R2_orig == max(R2_orig))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the elements' measurements of any of the species to use for random shuffling\n",
    "read_data = []\n",
    "for c in elements_names:\n",
    "    data = np.genfromtxt(HIs_data+\"/\"+str(c)+\".txt\" )\n",
    "    size = data.shape\n",
    "    read_data.append(data.ravel())\n",
    "read_data = np.array(read_data).T\n",
    "\n",
    "data_mask = np.zeros(read_data.shape[0])\n",
    "read_data_new = []\n",
    "for i in range(len(read_data)):\n",
    "    if len(np.where(read_data[i] > 0.)[0]) > 3:\n",
    "        read_data_new.append(read_data[i])\n",
    "        data_mask[i] = 1\n",
    "\n",
    "read_data_new = (np.array(read_data_new) - mean_X)/var_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat 500 times the prediction of the network replacing each input by random values\n",
    "model = Net(8,n_neur,1)\n",
    "loss_func = torch.nn.SmoothL1Loss()\n",
    "KK = 500\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y_H, test_size=0.20)\n",
    "y_test= torch.from_numpy(y_test.reshape(y_test.size,1)).float()\n",
    "\n",
    "all_err_total = np.zeros((KK, 8))\n",
    "model.load_state_dict(torch.load(save_folder+'nets/net_'+str(good_NN_inds[0])+'_N_'+str(n_neur)))\n",
    "y_pr = model(torch.from_numpy(X_test).float()) \n",
    "orig_loss = loss_func(y_test, y_pr.detach()).tolist()\n",
    "for jj in range(KK):\n",
    "    all_err1 = np.zeros((X_test.shape[1]))\n",
    "    for i in range(X_test.shape[1]):\n",
    "        X_perm = generate_X_perm(X_test, read_data_new, i)\n",
    "        y_pr = model(torch.from_numpy(X_perm).float())\n",
    "        all_err1[i] = loss_func(y_test, y_pr.detach()).tolist()\n",
    "    all_err_total[jj, :] = all_err1/orig_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.boxplot(all_err_total)\n",
    "plt.title(\"Importance of elements for the predicted hardness\", fontsize=20)\n",
    "plt.xticks(np.arange(len(elements_names))+1, elements_names, fontsize=18)\n",
    "plt.xlabel(\"Element\", fontsize=16)\n",
    "plt.ylabel(r'$MR_{el}$', fontsize=16)\n",
    "plt.savefig(save_folder+'importance_H.png', dpi=300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(save_folder+'importance_check_1NN_H.txt', all_err_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHAP for importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(8,n_neur,1)\n",
    "loss_func = torch.nn.SmoothL1Loss()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y_H, test_size=0.20)\n",
    "y_test= torch.from_numpy(y_test.reshape(y_test.size,1)).float()\n",
    "model.load_state_dict(torch.load(save_folder+'nets/net_'+str(good_NN_inds[0])+'_N_'+str(n_neur)))\n",
    "x_val = torch.from_numpy(X_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.DeepExplainer(model, torch.from_numpy(X_train).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(save_folder+'shap_values_H.txt', shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.boxplot(all_err_total)\n",
    "plt.title(\"Importance of elements for prediction with MR\", fontsize=18)\n",
    "plt.xticks(np.arange(len(elements_names))+1, elements_names, fontsize=18)\n",
    "plt.xlabel(\"Element\", fontsize=16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.ylabel(r'$MR_{el}$', fontsize=16)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.boxplot(np.abs(shap_values))\n",
    "plt.title(\"Importance of elements for prediction with SHAP\", fontsize=18)\n",
    "plt.xticks(np.arange(len(elements_names))+1, elements_names, fontsize=18)\n",
    "plt.xlabel(\"Element\", fontsize=16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.ylabel('SHAP', fontsize=16)\n",
    "\n",
    "#plt.savefig(save_folder+'MR_vs_SHAP_H_boxplots.png', dpi=300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "x = np.arange(8)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.bar(x, np.mean(all_err_total,axis=0)-1.0)\n",
    "plt.title(\"Importance of elements for prediction\", fontsize=20)\n",
    "plt.xticks(np.arange(len(elements_names)), elements_names, fontsize=18)\n",
    "plt.xlabel(\"Element\", fontsize=16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.ylabel(r'$MR_{el}$', fontsize=16)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.bar(x, np.mean(np.abs(shap_values), axis=0))\n",
    "plt.title(\"Importance of elements for prediction\", fontsize=20)\n",
    "plt.xticks(np.arange(len(elements_names)), elements_names, fontsize=18)\n",
    "plt.xlabel(\"Element\", fontsize=16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.ylabel('SHAP', fontsize=16)\n",
    "\n",
    "#plt.savefig(save_folder+'MR_vs_SHAP_H_barplots.png', dpi=300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot for publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.boxplot(all_err_total)\n",
    "plt.title(\"Importance of elements for prediction\", fontsize=20)\n",
    "plt.xticks(np.arange(len(elements_names))+1, elements_names, fontsize=18)\n",
    "plt.xlabel(\"Element\", fontsize=16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.ylabel(r'$MR_{el}$', fontsize=16)\n",
    "\n",
    "\n",
    "ax1 = plt.subplot(133)\n",
    "plt.title(\"Neural networks performance\", fontsize=20)\n",
    "c = 'k'\n",
    "ax1.set_xlabel(\"Number of neurons in hidden layer\", fontsize=16)\n",
    "ax1.set_ylabel(r'$R^2$', fontsize=16, color=c)\n",
    "plt.boxplot(R2_all.T,notch=True, patch_artist=True, boxprops=dict(facecolor=\"white\", color=c),\n",
    "            capprops=dict(color=c),\n",
    "            whiskerprops=dict(color=c),\n",
    "            flierprops=dict(color=c, markeredgecolor=c),\n",
    "            medianprops=dict(color=c),) \n",
    "ax1.tick_params(axis='y', labelcolor=c)\n",
    "plt.ylabel(r'$R^2$', fontsize=16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.xticks(np.arange(len(Ns))+1, Ns, fontsize = 16)\n",
    "\n",
    "ax2 = ax1.twinx()  \n",
    "c = 'r'\n",
    "ax2.set_ylabel(r'Smooth $L_1$ loss', fontsize=16, color=c)  \n",
    "plt.boxplot(Loss_all.T,notch=True, patch_artist=True, boxprops=dict(facecolor=\"white\", color=c),\n",
    "            capprops=dict(color=c),\n",
    "            whiskerprops=dict(color=c),\n",
    "            flierprops=dict(color=c, markeredgecolor=c),\n",
    "            medianprops=dict(color=c),) \n",
    "ax2.tick_params(axis='y', labelcolor=c)\n",
    "plt.xlabel(\"Number of neurons in hidden layer\", fontsize=16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.xticks(np.arange(len(Ns))+1, Ns, fontsize = 16)\n",
    "\n",
    "plt.subplot(132)\n",
    "\n",
    "multi_class = False\n",
    "features = X_test[:]\n",
    "color = \"coolwarm\"\n",
    "layered_violin_max_num_bins=20\n",
    "axis_color=\"#333333\"\n",
    "num_features = shap_values.shape[1]\n",
    "feature_names = elements_names\n",
    "max_display = 20\n",
    "\n",
    "feature_order = np.argsort(np.sum(np.abs(shap_values), axis=0))\n",
    "feature_order = feature_order[-min(max_display, len(feature_order)):]\n",
    "\n",
    "plt.axvline(x=0, color=\"#999999\", zorder=-1)\n",
    "max_display = min(len(feature_names), max_display)\n",
    "sort_inds = np.argsort(-np.abs(shap_values.sum(1)).sum(0))\n",
    "\n",
    "# get plotting limits\n",
    "delta = 1.0 / (shap_values.shape[1] ** 2)\n",
    "slow = np.nanpercentile(shap_values, delta)\n",
    "shigh = np.nanpercentile(shap_values, 100 - delta)\n",
    "v = max(abs(slow), abs(shigh))\n",
    "slow = -v\n",
    "shigh = v\n",
    "\n",
    "num_x_points = 200\n",
    "bins = np.linspace(0, features.shape[0], layered_violin_max_num_bins + 1).round(0).astype('int') \n",
    "shap_min, shap_max = np.min(shap_values), np.max(shap_values)\n",
    "x_points = np.linspace(shap_min, shap_max, num_x_points)\n",
    "plt.axvline(x=0, color=\"#999999\", zorder=-1)\n",
    "# loop through each feature and plot:\n",
    "for pos, ind in enumerate(feature_order):\n",
    "    feature = features[:, ind]\n",
    "    unique, counts = np.unique(feature, return_counts=True)\n",
    "    if unique.shape[0] <= layered_violin_max_num_bins:\n",
    "        order = np.argsort(unique)\n",
    "        thesebins = np.cumsum(counts[order])\n",
    "        thesebins = np.insert(thesebins, 0, 0)\n",
    "    else:\n",
    "        thesebins = bins\n",
    "    nbins = thesebins.shape[0] - 1\n",
    "    order = np.argsort(feature)\n",
    "    y0 = np.ones(num_x_points) * pos\n",
    "    ys = np.zeros((nbins, num_x_points))\n",
    "    for i in range(nbins):\n",
    "        shaps = shap_values[order[thesebins[i]:thesebins[i + 1]], ind]\n",
    "        ys[i, :] = gaussian_kde(shaps + np.random.normal(loc=0, scale=0.001, size=shaps.shape[0]))(x_points)\n",
    "        size = thesebins[i + 1] - thesebins[i]\n",
    "        bin_size_if_even = features.shape[0] / nbins\n",
    "        relative_bin_size = size / bin_size_if_even\n",
    "        ys[i, :] *= relative_bin_size\n",
    "    ys = np.cumsum(ys, axis=0)\n",
    "    width = 0.8\n",
    "    scale = ys.max() * 2 / width  \n",
    "    for i in range(nbins - 1, -1, -1):\n",
    "        y = ys[i, :] / scale\n",
    "        c = plt.get_cmap(color)(i / (nbins - 1.)) if color in plt.cm.datad else color\n",
    "        plt.fill_between(x_points, pos - y, pos + y, facecolor=c)\n",
    "plt.xlim(shap_min, shap_max)\n",
    "\n",
    "m = cm.ScalarMappable(cmap=plt.get_cmap(color))\n",
    "m.set_array([0, 1])\n",
    "cb = plt.colorbar(m, ticks=[0, 1], aspect=1000)\n",
    "cb.set_ticklabels(['Low', \"High\"])\n",
    "cb.set_label(\"Feature value\", size=16, labelpad=-4)\n",
    "cb.ax.tick_params(labelsize=16, length=0)\n",
    "cb.set_alpha(1)\n",
    "cb.outline.set_visible(False)\n",
    "bbox = cb.ax.get_window_extent().transformed(plt.gcf().dpi_scale_trans.inverted())\n",
    "cb.ax.set_aspect((bbox.height - 0.9) * 20)\n",
    "\n",
    "plt.gca().xaxis.set_ticks_position('bottom')\n",
    "plt.gca().yaxis.set_ticks_position('none')\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().tick_params(color=axis_color, labelcolor=axis_color)\n",
    "plt.yticks(range(len(feature_order)), [feature_names[i] for i in feature_order], fontsize=13)\n",
    "plt.gca().tick_params('both', labelsize=16)\n",
    "plt.ylim(-1, len(feature_order))\n",
    "plt.xlabel(\"Impact on model output\", fontsize=16)\n",
    "plt.title(\"SHAP\", fontsize=18)\n",
    "\n",
    "plt.savefig(save_folder+'Imp_SHAP_R2_H.png', dpi=300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
